---
title: Logs Performance Bencmark
slug: logs-perf
date: 2022-11-21
tags: [Perf Bencmark]
authors: [nitya]
description: Performance benchmark comparison for logs in SigNoz, ElasticSearch and Loki
image: /img/blog/2022/11/winston_logger_cover.webp
keywords:
  - winston logger
  - nodejs
  - log management
---
<head>
  <link rel="canonical" href="https://signoz.io/blog/logs-perf/"/>
</head>

Logs are an integral part of any system that helps you get information about the applications state, and how it is handling its operations. The goal of this blog is to compare the commonly used logging solutions i.e ElasticSearch(ELK stack) and Loki(PLG stack) with SigNoz. We will go through how we created the environment for benchmarking the different solutions and compare the results which will help to choose a right logging solution.
<!--truncate-->

<!-- ![Cover Image](/img/blog/2022/11/winston_logger_cover.webp) -->

<!-- trunk-ignore(markdownlint/MD026) -->
## Benchmark Preparation and Setup.
For the comparsion we have disqualified DataDog as it is a cloud based solution and we will be only comparing with **ELK**(Elasticsearch Loki and Kibana) stack and **PLG**(Promtail Loki Grafana) stack

For generating logs we will be using [flog](https://github.com/signoz/flog). Through this tool we will be able to generate logs where each log size is greater than 1.5 KB and less than 2.0 KB.


Here is a sample log generated by flog
<!-- trunk-ignore(markdownlint/MD040) -->
```
{
  "host": "6.127.39.239",
  "user-identifier": "wisoky5015",
  "datetime": "09/Nov/2022:04:48:14 +0000",
  "method": "HEAD",
  "request": "/revolutionary/roi",
  "protocol": "HTTP/2.0",
  "status": 501,
  "bytes": 11466,
  "referer": "http://www.seniorrevolutionize.name/intuitive/matrix/24/7",
  "traceId": "3aee4c94836ffe064b37f054dc0e1c50",
  "spanId": "4b37f054dc0e1c50",
  "dummyBytes": "kingship grumble swearer hording wiggling dipper execution's crock's clobbers Spaniard's priestess's organises Harrods's Josef's Wilma meditating movable's photographers polestars pectorals Coventries rehearsal Arkhangelsk's Kasai barometer junkier Osgood's impassively illogical cardsharp's underbrush shorter patronymics plagiarises gargle's Chandra intransigent deathtrap extraneously hairless inferred glads frail polka jeez ohm bigoted sari's cannonades vestibule's pursuer vanity ceremony Kodaly's swaths disturbance's belt Samoan's invulnerability dumping Alfreda padded corrosive universal dishwasher's multiplier decisive selloff eastbound moods Konrad's appositive's reset's ingenuously checkup's overselling evens Darrin's supernumerary liberalism productivity's legrooms Lorenzo including Robbin savourier foxglove's buckshot businesswomen abalones tare Chaitin Stephenson's inpatients distinction cryings aspic empire's healed perspiring"
}
```

For deploying and benchmarking all the three logging solutions we will use four VM's. Three of the VM's will be used for generating logs and one will be used for deploying the logging solution.
The VM used for deploying the logging solution is a **`c6a.4xlarge`** EC2 instance which has 16vCPU, 32GB of RAM and network bandwidth upto 12.5 Gbps. While the three VM's for generating logs is **`c6a.2xlarge`** EC2 instance which has 8vCPU, 16GB of RAM and network bandwidth of upto 12.5 Gbps.

All the configurations used for performing the benchmark can be found here https://github.com/SigNoz/logs-benchmark.

### Preparing Signoz
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-setup.png" alt="SigNoz setup for logs performance benchmark"/>
    <figcaption><i>SigNoz setup for logs performance benchmark</i></figcaption>
</figure>
The three VM's and generating logs and sending it to signoz-otel-collector using OTLP. The reason we are using OTEL collectors on the receiver side instead of directly writing to clickhouse from the generator VM's is that the OTEL collectors running in the generator VM's may or maynot be the distribution from SigNoz.

In our otel collector configuration we have extraced all fields and we have converted all the fields to interesting fields in the UI which means they all are indexed.

### Preparing Elasticsearch
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elastic-logs-setup.png" alt="ElasticSearch setup for logs performance benchmark"/>
    <figcaption><i>ElasticSearch setup for logs performance benchmark</i></figcaption>
</figure>
In the three VM's the logs that are getting generated are getting directly written to elasticsearch deployed in the host VM. Here elasticsearch is using dynamic indexing template to index all the fields present in the logs.

### Preparing Loki
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/loki-logs-setup.png" alt="Loki setup for logs performance benchmark"/>
    <figcaption><i>Loki setup for logs performance benchmark</i></figcaption>
</figure>

Here we have used two flog containers for generating logs since when we try to increase the flog containers Loki errors out with **max number of streams** error. Here we also weren't able to index anything apart from two attributes which are **protocol and method**. If we try to create more labels it reaches max stream error.

It is also recommended by Grafana to keep the number of labels as low as possible and not to index high cardinality data.

## Benchmark Results

### Ingestion Benchmark Results

The table below represents the summary for ingesting 500GB of logs data in each of the logging solutions

| Name    | Ingestion time  | Indexes             |
|---------|-----------------|---------------------|
| SigNoz  | 2 hours         | All Keys            |
| Elastic | 5 hours 27 mins | All Keys            |
| Loki    | 3 hours 20 mins | Method and Protocol |

Now lets go through each of them individually.
#### SigNoz
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-insertion-speed.png" alt="SigNoz insertion speed count per sec"/>
    <figcaption><i>SigNoz insertion speed(count/sec)</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-insertion-bytes.png" alt="SigNoz insertion speed bytes por sec"/>
    <figcaption><i>SigNoz insertion speed(bytes/sec)</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-insertion-cpu.png" alt="SigNoz CPU usage"/>
    <figcaption><i>SigNoz VM using 40% of the CPU</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-insertion-memory.png" alt="SigNoz Memory Usage"/>
    <figcaption><i>SigNoz VM using 20% of the available memory</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/signoz-logs-insertion-diskio.png" alt="SigNoz Disk I/O"/>
    <figcaption><i>SigNoz VM Disk I/O</i></figcaption>
</figure>
From the above graphs we can see that 500GB of logs data ingested in 2 hours. The ingestion rate was 55K logs/second or 75 MiB/s.
The CPU usage during the ingestion was about 40% and the memory usage was about 20%.

#### ElasticSearch
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elk-indexing-rate.png" alt="ElasticSearh indexing rate"/>
    <figcaption><i>ElasticSearch indexing rate.</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elk-translog-operations.png" alt="ElasticSearch translog operations"/>
    <figcaption><i>ElasticSearch translog operations.</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elk-cpu.png" alt="ElasticSearch CPU usage"/>
    <figcaption><i>ElasticSearch VM using 75% of the CPU</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elk-memory.png" alt="ElasticSearch Memory Usage"/>
    <figcaption><i>ElasticSearch VM using 60% of the available memory</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/elk-diskio.png" alt="ElasticSearch Disk I/O"/>
    <figcaption><i>ElasticSearch VM Disk I/O</i></figcaption>
</figure>

For ElasticSearch 500 GB of logs data ingested in 5 hours 27 mins with an average indexing rate of 20K/s.   
Average translog rate of 8.4K/s and tanslog size with average 23.3MiB/s. The CPU usage was around 75% and the memory usage was about 60%

#### Loki
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/loki-ingestion.png" alt="Loki insertion speed count per sec and bytes per sec"/>
    <figcaption><i>Loki insertion speed(count/sec and bytes/sec)</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/loki-cpu.png" alt="Loki CPU usage"/>
    <figcaption><i>Loki VM using 15% of the CPU</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/loki-memory.png" alt="Loki Memory Usage"/>
    <figcaption><i>Loki VM using 8.3% of the available memory</i></figcaption>
</figure>
<figure data-zoomable align='center'>
    <img src="/img/blog/2022/11/loki-diskio.png" alt="Loki Disk I/O"/>
    <figcaption><i>Loki VM Disk I/O</i></figcaption>
</figure>

500 GB logs ingested in 3 hours and 20 mins with an ingestion rate of 21K logs/s and size of 29 MB/s.

### Query Benchmark Results

| Query                                                             | SigNoz | ElasticSearch | Loki |
|-------------------------------------------------------------------|--------|---------------|------|
| Average value of the attribute bytes in the entire data           | 0.48s  | 6.489s        | -    |
| logs count with method GET per 10 min over the entire data        | 2.12s  | 0.438s        | -    |
| logs count for a user-identifier per 10 min over the entire data  | 0.15s  | 0.002s        | -    |
| Get logs corresponding to a trace_id                              | 0.137s | 0.001s        | -    |
| Get first 100 logs with method GET                                | 0.20s  | 0.014s        | -    |


## Storage Comparison

| Name          | Space Used | Document Count       |
|---------------|------------|----------------------|
| SigNoz        | 207.9G     | 382mil               |
| ElasticSearch | 388G       | 376mil               |
| Loki          | 142.2G     | unknown(query fails) |


## Conclusion
- For ingestion SigNoz is 2.5x faster than ELK and uses 50% less resources.
- SigNoz is about **14 times** faster than ELK for aggregation queries.
- **Storage** used by SigNoz for the same amount of logs is **half of what ELK uses.**
- Loki is not a tool to be used when you want to index and query high cardinality data.
